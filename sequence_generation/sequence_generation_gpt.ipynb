{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Sentence Generation from Language Model\n",
    "\n",
    "This tutorial demonstrates how to generate text using a pre-trained language model in the following two ways:\n",
    "\n",
    "- with sequence sampler\n",
    "- with beam search sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Variables to configure when generating sequences:\n",
    "\n",
    "- V = vocabulary size\n",
    "- T = sequence length\n",
    "- the number of possible outcomes to consider a sequence = V^T."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Given a language model, we can generate sequences according to the probability that they would occur according to our model. At each time step, a language model predicts the likelihood of each word occuring, given the context from prior time steps. The outputs at any time step can be any word from the vocabulary whose size is V and thus the number of all possible outcomes for a sequence of length T is thus V^T. \n",
    "\n",
    "While sometimes we might want to generate sentences according to their probability of occuring, at other times we want to find the sentences that *are most likely to occur*. This is especially true in the case of language translation where we don't just want to see *a* translation. We want the *best* translation. While finding the optimal outcome quickly becomes intractable as time step increases, there are still many ways to sample reasonably good sequences. GluonNLP provides two samplers for generating from a language model: SequenceSampler and BeamSearchSampler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "First import the libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mxnet as mx\n",
    "import gluonnlp as nlp\n",
    "import text_generation.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Load Pretrained Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "# change to mx.cpu() if GPU is not present\n",
    "ctx = mx.cpu()\n",
    "\n",
    "model, vocab = text_generation.model.get_model(name='gpt2_117m',\n",
    "                                               dataset_name='openai_webtext',\n",
    "                                               pretrained=True,\n",
    "                                               ctx=ctx)\n",
    "tokenizer = nlp.data.GPT2BPETokenizer()\n",
    "detokenizer = nlp.data.GPT2BPEDetokenizer()\n",
    "\n",
    "eos_id = vocab[vocab.eos_token]\n",
    "print(vocab.eos_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Sampling a Sequence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sequence Sampler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "A SequenceSampler samples from the contextual multinomial distribution produced by the language model at each time step. Since we may want to control how \"sharp\" the distribution is to tradeoff diversity with correctness, we can use the temperature option in SequenceSampler, which controls the temperature of the softmax function.\n",
    "\n",
    "For each input same, sequence sampler can sample multiple independent sequences at once. The number of independent sequences to sample can be specified through the argument `beam_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'\\u0120Deep', u'\\u0120learning', u'\\u0120and', u'\\u0120natural', u'\\u0120language', u'\\u0120processing']\n"
     ]
    }
   ],
   "source": [
    "bos_str = 'Deep learning and natural language processing'\n",
    "if not bos_str.startswith(' '):\n",
    "    bos_str = ' ' + bos_str\n",
    "bos_tokens = tokenizer(bos_str)\n",
    "bos_ids = vocab[bos_tokens]\n",
    "print(bos_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Define the Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class GPT2Decoder(text_generation.model.LMDecoder):\n",
    "    def __call__(self, inputs, states):\n",
    "        inputs = inputs.expand_dims(axis=1)\n",
    "        out, new_states = self.net(inputs, states)\n",
    "        out = mx.nd.slice_axis(out, axis=1, begin=0, end=1).reshape((inputs.shape[0], -1))\n",
    "        return out, new_states\n",
    "    \n",
    "decoder = GPT2Decoder(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Define the initial state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def get_initial_input_state(decoder, bos_ids, temperature):\n",
    "    inputs, begin_states = decoder.net(\n",
    "        mx.nd.array([bos_ids], dtype=np.int32, ctx=ctx), None)\n",
    "    inputs = inputs[:, -1, :]\n",
    "    smoothed_probs = (inputs / temperature).softmax(axis=1)\n",
    "    inputs = mx.nd.sample_multinomial(smoothed_probs, dtype=np.int32)\n",
    "    return inputs, begin_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Define the Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# number of independent sequences to search\n",
    "beam_size = 2\n",
    "temperature = 0.97\n",
    "num_results = 2\n",
    "# must be less than 1024\n",
    "max_len = 256 - len(bos_tokens)\n",
    "sampler = nlp.model.SequenceSampler(beam_size=beam_size,\n",
    "                                    decoder=decoder,\n",
    "                                    eos_id=eos_id,\n",
    "                                    max_length=max_len,\n",
    "                                    temperature=temperature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Generate result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def generate(decoder, bos_ids, temperature, sampler, num_results, vocab):\n",
    "    inputs, begin_states = get_initial_input_state(decoder, bos_ids, temperature)\n",
    "    # samples have shape (1, beam_size, length), scores have shape (1, beam_size)\n",
    "    samples, scores, valid_lengths = sampler(inputs, begin_states)\n",
    "    samples = samples[0].asnumpy()\n",
    "    scores = scores[0].asnumpy()\n",
    "    valid_lengths = valid_lengths[0].asnumpy()\n",
    "\n",
    "    print('Generation Result:')\n",
    "    for i in range(num_results):\n",
    "        generated_tokens = [vocab.idx_to_token[ele] for ele in samples[i][:valid_lengths[i]]]\n",
    "        tokens = bos_tokens + generated_tokens[1:]\n",
    "        print([detokenizer(tokens).strip(), scores[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation Result:\n",
      "[u'Deep learning and natural language processing serious improvements over existing programming languages.\\n\\nNo framework or language developers have yet heard about the urgent need for deep learning--but so far, there is little that is going on. New Scientist has written new or rewritten research on how deep learning can be applied directly to machine learning new projects. So far, we\\'ve wrestled to learn how algorithms may play a key design factors causing particular events so closely related to machine learning and linguistics, but, research only reveal which systems can be used to inform behavior. This approach is unfortunate for theoretical approaches to predictive business matters of machine learning. Developers may feel worked on most training so far better. The gap hole-in\\'s reach, compress 3.\\n\\n\\n\\n\\n\\n\\nHe may buy deeper, these at cold brutal people.\\nAlienance Aplots, but learned analyze seriously\\nNew scientist and know new structural frameworks have huge robot life, wealth depletion\\nHistorically emotional workings is new while here work is plentiful then you\\nMr Hackintosh value MULT index do financial calls quality more information 125Cens terrifying algorithms good time being\" new music pronounced Good knew too battery and messaging on robotics know increasingly in Block brand is use algorithm denied offers acrocs grow invested in coo<|endoftext|>', -1295.8932]\n",
      "[u'Deep learning and natural language processing researchers in their last decade. Nature Neuroscience (2011). DOI: 10.1038/nnmonitor.2011.841\\n\\nhttp://www.nature.com/nature/articles/npr086191\\n\\n\\n\\u2013\\nReprint2014-01-08-24D.\\n\\n\\n[~Berry, October 4<|endoftext|>', -188.74075]\n"
     ]
    }
   ],
   "source": [
    "generate(decoder, bos_ids, temperature, sampler, num_results, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Beam Search Sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "To overcome the exponential complexity in sequence decoding, beam search decodes greedily, keeping those sequences that are most likely based on the probability up to the current time step. The size of this subset is called the *beam size*. Suppose the beam size is K and the output vocabulary size is V. When selecting the beams to keep, the beam search algorithm first predict all possible successor words from the previous K beams, each of which has V possible outputs. This becomes a total of K\\*V paths. Out of these K\\*V paths, beam search ranks them by their score keeping only the top K paths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### Generate Sequences w/ Beam Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Next, we are going to generate sentences starting with \"I love it\" using beam search first. We feed ['I', 'Love'] to the language model to get the initial states and set the initial input to be the word 'it'. We will then print the top-3 generations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Scorer Function\n",
    "\n",
    "The BeamSearchScorer is a simple HybridBlock that implements the scoring function with length penalty in Google NMT paper. \n",
    "```\n",
    "scores = (log_probs + scores) / length_penalty\n",
    "length_penalty = (K + length)^alpha / (K + 1)^alpha\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "scorer = nlp.model.BeamSearchScorer(alpha=0, K=5, from_logits=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Beam Search Sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Given a scorer and decoder, we are ready to create a sampler. We use symbol '.' to indicate the end of sentence (EOS). We can use vocab to get the index of the EOS, and then feed the index to the sampler. The following codes shows how to construct a beam search sampler. We will create a sampler with 4 beams and a maximum sample length of 20.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "beam_sampler = nlp.model.BeamSearchSampler(beam_size=3,\n",
    "                                           decoder=decoder,\n",
    "                                           eos_id=eos_id,\n",
    "                                           scorer=scorer,\n",
    "                                           max_length=max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Generate Sequences w/ Sequence Sampler\n",
    "Now, use the sequence sampler created to sample sequences based on the same inputs used previously.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation Result:\n",
      "[u'Deep learning and natural language processing.\\n\\nIn this paper, we present the results of a systematic review and meta-analysis of the literature on natural language processing and natural language processing in humans. We present the results of a systematic review and meta-analysis of the literature on natural language processing and natural language processing in humans. We present the results of a systematic review and meta-analysis of the literature on natural language processing and meta-analysis of the literature on natural language processing and meta-analysis of literature on natural language processing and meta-analysis of literature on meta-analysis of literature on meta-analysis of literature on meta-analysis of literature on meta-analysis of meta-analysis of meta-analysis of literature on meta-analysis of meta-analysis of meta-analysis of meta-analysis of meta-analysis of literature on meta-analysis of meta-analysis of meta-analysis of literature on meta-analysis of meta-analysis of meta-analysis of literature on meta-analysis of meta-analysis of meta-analysis of literature on meta-analysis of meta-analysis of meta-analysis of literature on meta-analysis of meta-analysis of meta-analysis of literature on meta-analysis of meta-analysis of literature on meta-analysis of meta-analysis of meta-analysis of meta-analysis of<|endoftext|>', -126.39271]\n",
      "[u'Deep learning and natural language processing.\\n\\nIn this paper, we present the results of a systematic review and meta-analysis of the literature on natural language processing and natural language processing in humans. We present the results of a systematic review and meta-analysis of the literature on natural language processing and natural language processing in humans. We present the results of a systematic review and meta-analysis of the literature on natural language processing and meta-analysis of the literature on natural language processing and meta-analysis of literature on natural language processing and meta-analysis of literature on meta-analysis of literature on meta-analysis of literature on meta-analysis of literature on meta-analysis of meta-analysis of meta-analysis of literature on meta-analysis of meta-analysis of meta-analysis of meta-analysis of meta-analysis of literature on meta-analysis of meta-analysis of meta-analysis of literature on meta-analysis of meta-analysis of meta-analysis of literature on meta-analysis of meta-analysis of meta-analysis of literature on meta-analysis of meta-analysis of meta-analysis of literature on meta-analysis of meta-analysis of meta-analysis of literature on meta-analysis of meta-analysis of literature on meta-analysis of meta-analysis of meta-analysis of metaanalysis of meta<|endoftext|>', -127.03417]\n"
     ]
    }
   ],
   "source": [
    "generate(decoder, bos_ids, temperature, beam_sampler, num_results, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Practice\n",
    "\n",
    "- Tweak alpha and K in BeamSearchScorer, how are the results changed?\n",
    "- Try different samples to decode."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "rise": {
   "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
