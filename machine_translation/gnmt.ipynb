{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Neural Machine Translation\n",
    "\n",
    "In this notebook, we are going to train Google NMT on IWSLT 2015 English-Vietnamese\n",
    "Dataset. The building process includes four steps: 1) load and process dataset, 2)\n",
    "create sampler and DataLoader, 3) build model, and 4) write training epochs.\n",
    "\n",
    "## Load MXNET and Gluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "import io\n",
    "import logging\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet import gluon\n",
    "import gluonnlp as nlp\n",
    "import nmt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Logs will be saved to gnmt_en_vi_u512/<ipython-input-2-4699ac3a1bfb>.log\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'gnmt_en_vi_u512'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(100)\n",
    "random.seed(100)\n",
    "mx.random.seed(10000)\n",
    "ctx = mx.gpu(0)\n",
    "\n",
    "# parameters for dataset\n",
    "dataset = 'IWSLT2015'\n",
    "src_lang, tgt_lang = 'en', 'vi'\n",
    "src_max_len, tgt_max_len = 50, 50\n",
    "\n",
    "# parameters for model\n",
    "num_hidden = 512\n",
    "num_layers = 2\n",
    "num_bi_layers = 1\n",
    "dropout = 0.2\n",
    "\n",
    "# parameters for training\n",
    "batch_size, test_batch_size = 128, 32\n",
    "num_buckets = 5\n",
    "epochs = 1\n",
    "clip = 5\n",
    "lr = 0.001\n",
    "lr_update_factor = 0.5\n",
    "log_interval = 10\n",
    "save_dir = 'gnmt_en_vi_u512'\n",
    "\n",
    "#parameters for testing\n",
    "beam_size = 10\n",
    "lp_alpha = 1.0\n",
    "lp_k = 5\n",
    "\n",
    "nmt.utils.logging_config(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Preprocess Dataset\n",
    "\n",
    "The following shows how to process the dataset and cache the processed dataset\n",
    "for future use. The processing steps include: 1) clip the source and target\n",
    "sequences, 2) split the string input to a list of tokens, 3) map the\n",
    "string token into its integer index in the vocabulary, and 4) append end-of-sentence (EOS) token to source\n",
    "sentence and add BOS and EOS tokens to target sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading /var/lib/jenkins/workspace/gluon-nlp-gpu-py3@2/tests/data/datasets/iwslt2015/iwslt15.zip from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/iwslt2015/iwslt15.zip...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing time spent: 5.375916004180908\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing time spent: 0.06687569618225098\n",
      "Processing time spent: 0.06173110008239746\n"
     ]
    }
   ],
   "source": [
    "def cache_dataset(dataset, prefix):\n",
    "    \"\"\"Cache the processed npy dataset  the dataset into a npz\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : gluon.data.SimpleDataset\n",
    "    file_path : str\n",
    "    \"\"\"\n",
    "    if not os.path.exists(nmt._constants.CACHE_PATH):\n",
    "        os.makedirs(nmt._constants.CACHE_PATH)\n",
    "    src_data = np.concatenate([e[0] for e in dataset])\n",
    "    tgt_data = np.concatenate([e[1] for e in dataset])\n",
    "    src_cumlen = np.cumsum([0]+[len(e[0]) for e in dataset])\n",
    "    tgt_cumlen = np.cumsum([0]+[len(e[1]) for e in dataset])\n",
    "    np.savez(os.path.join(nmt._constants.CACHE_PATH, prefix + '.npz'),\n",
    "             src_data=src_data, tgt_data=tgt_data,\n",
    "             src_cumlen=src_cumlen, tgt_cumlen=tgt_cumlen)\n",
    "\n",
    "\n",
    "def load_cached_dataset(prefix):\n",
    "    cached_file_path = os.path.join(nmt._constants.CACHE_PATH, prefix + '.npz')\n",
    "    if os.path.exists(cached_file_path):\n",
    "        print('Load cached data from {}'.format(cached_file_path))\n",
    "        npz_data = np.load(cached_file_path)\n",
    "        src_data, tgt_data, src_cumlen, tgt_cumlen = [npz_data[n] for n in\n",
    "                ['src_data', 'tgt_data', 'src_cumlen', 'tgt_cumlen']]\n",
    "        src_data = np.array([src_data[low:high] for low, high in zip(src_cumlen[:-1], src_cumlen[1:])])\n",
    "        tgt_data = np.array([tgt_data[low:high] for low, high in zip(tgt_cumlen[:-1], tgt_cumlen[1:])])\n",
    "        return gluon.data.ArrayDataset(np.array(src_data), np.array(tgt_data))\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "class TrainValDataTransform(object):\n",
    "    \"\"\"Transform the machine translation dataset.\n",
    "\n",
    "    Clip source and the target sentences to the maximum length. For the source sentence, append the\n",
    "    EOS. For the target sentence, append BOS and EOS.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    src_vocab : Vocab\n",
    "    tgt_vocab : Vocab\n",
    "    src_max_len : int\n",
    "    tgt_max_len : int\n",
    "    \"\"\"\n",
    "    def __init__(self, src_vocab, tgt_vocab, src_max_len, tgt_max_len):\n",
    "        self._src_vocab = src_vocab\n",
    "        self._tgt_vocab = tgt_vocab\n",
    "        self._src_max_len = src_max_len\n",
    "        self._tgt_max_len = tgt_max_len\n",
    "\n",
    "    def __call__(self, src, tgt):\n",
    "        if self._src_max_len > 0:\n",
    "            src_sentence = self._src_vocab[src.split()[:self._src_max_len]]\n",
    "        else:\n",
    "            src_sentence = self._src_vocab[src.split()]\n",
    "        if self._tgt_max_len > 0:\n",
    "            tgt_sentence = self._tgt_vocab[tgt.split()[:self._tgt_max_len]]\n",
    "        else:\n",
    "            tgt_sentence = self._tgt_vocab[tgt.split()]\n",
    "        src_sentence.append(self._src_vocab[self._src_vocab.eos_token])\n",
    "        tgt_sentence.insert(0, self._tgt_vocab[self._tgt_vocab.bos_token])\n",
    "        tgt_sentence.append(self._tgt_vocab[self._tgt_vocab.eos_token])\n",
    "        src_npy = np.array(src_sentence, dtype=np.int32)\n",
    "        tgt_npy = np.array(tgt_sentence, dtype=np.int32)\n",
    "        return src_npy, tgt_npy\n",
    "\n",
    "\n",
    "def process_dataset(dataset, src_vocab, tgt_vocab, src_max_len=-1, tgt_max_len=-1):\n",
    "    start = time.time()\n",
    "    dataset_processed = dataset.transform(TrainValDataTransform(src_vocab, tgt_vocab,\n",
    "                                                                src_max_len,\n",
    "                                                                tgt_max_len), lazy=False)\n",
    "    end = time.time()\n",
    "    print('Processing time spent: {}'.format(end - start))\n",
    "    return dataset_processed\n",
    "\n",
    "\n",
    "def load_translation_data(dataset, src_lang='en', tgt_lang='vi'):\n",
    "    \"\"\"Load translation dataset\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : str\n",
    "    src_lang : str, default 'en'\n",
    "    tgt_lang : str, default 'vi'\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data_train_processed : Dataset\n",
    "        The preprocessed training sentence pairs\n",
    "    data_val_processed : Dataset\n",
    "        The preprocessed validation sentence pairs\n",
    "    data_test_processed : Dataset\n",
    "        The preprocessed test sentence pairs\n",
    "    val_tgt_sentences : list\n",
    "        The target sentences in the validation set\n",
    "    test_tgt_sentences : list\n",
    "        The target sentences in the test set\n",
    "    src_vocab : Vocab\n",
    "        Vocabulary of the source language\n",
    "    tgt_vocab : Vocab\n",
    "        Vocabulary of the target language\n",
    "    \"\"\"\n",
    "    common_prefix = 'IWSLT2015_{}_{}_{}_{}'.format(src_lang, tgt_lang,\n",
    "                                                   src_max_len, tgt_max_len)\n",
    "    data_train = nlp.data.IWSLT2015('train', src_lang=src_lang, tgt_lang=tgt_lang)\n",
    "    data_val = nlp.data.IWSLT2015('val', src_lang=src_lang, tgt_lang=tgt_lang)\n",
    "    data_test = nlp.data.IWSLT2015('test', src_lang=src_lang, tgt_lang=tgt_lang)\n",
    "    src_vocab, tgt_vocab = data_train.src_vocab, data_train.tgt_vocab\n",
    "    data_train_processed = load_cached_dataset(common_prefix + '_train')\n",
    "    if not data_train_processed:\n",
    "        data_train_processed = process_dataset(data_train, src_vocab, tgt_vocab,\n",
    "                                               src_max_len, tgt_max_len)\n",
    "        cache_dataset(data_train_processed, common_prefix + '_train')\n",
    "    data_val_processed = load_cached_dataset(common_prefix + '_val')\n",
    "    if not data_val_processed:\n",
    "        data_val_processed = process_dataset(data_val, src_vocab, tgt_vocab)\n",
    "        cache_dataset(data_val_processed, common_prefix + '_val')\n",
    "    data_test_processed = load_cached_dataset(common_prefix + '_test')\n",
    "    if not data_test_processed:\n",
    "        data_test_processed = process_dataset(data_test, src_vocab, tgt_vocab)\n",
    "        cache_dataset(data_test_processed, common_prefix + '_test')\n",
    "    fetch_tgt_sentence = lambda src, tgt: tgt.split()\n",
    "    val_tgt_sentences = list(data_val.transform(fetch_tgt_sentence))\n",
    "    test_tgt_sentences = list(data_test.transform(fetch_tgt_sentence))\n",
    "    return data_train_processed, data_val_processed, data_test_processed, \\\n",
    "           val_tgt_sentences, test_tgt_sentences, src_vocab, tgt_vocab\n",
    "\n",
    "\n",
    "def get_data_lengths(dataset):\n",
    "    return list(dataset.transform(lambda srg, tgt: (len(srg), len(tgt))))\n",
    "\n",
    "\n",
    "data_train, data_val, data_test, val_tgt_sentences, test_tgt_sentences, src_vocab, tgt_vocab\\\n",
    "    = load_translation_data(dataset=dataset, src_lang=src_lang, tgt_lang=tgt_lang)\n",
    "data_train_lengths = get_data_lengths(data_train)\n",
    "data_val_lengths = get_data_lengths(data_val)\n",
    "data_test_lengths = get_data_lengths(data_test)\n",
    "\n",
    "with io.open(os.path.join(save_dir, 'val_gt.txt'), 'w', encoding='utf-8') as of:\n",
    "    for ele in val_tgt_sentences:\n",
    "        of.write(' '.join(ele) + '\\n')\n",
    "\n",
    "with io.open(os.path.join(save_dir, 'test_gt.txt'), 'w', encoding='utf-8') as of:\n",
    "    for ele in test_tgt_sentences:\n",
    "        of.write(' '.join(ele) + '\\n')\n",
    "\n",
    "\n",
    "data_train = data_train.transform(lambda src, tgt: (src, tgt, len(src), len(tgt)), lazy=False)\n",
    "data_val = gluon.data.SimpleDataset([(ele[0], ele[1], len(ele[0]), len(ele[1]), i)\n",
    "                                     for i, ele in enumerate(data_val)])\n",
    "data_test = gluon.data.SimpleDataset([(ele[0], ele[1], len(ele[0]), len(ele[1]), i)\n",
    "                                      for i, ele in enumerate(data_test)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Sampler and DataLoader\n",
    "\n",
    "Now, we have obtained `data_train`, `data_val`, and `data_test`. The next step\n",
    "is to construct sampler and DataLoader. The first step is to construct batchify\n",
    "function, which pads and stacks sequences to form mini-batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batchify_fn = nlp.data.batchify.Tuple(nlp.data.batchify.Pad(),\n",
    "                                            nlp.data.batchify.Pad(),\n",
    "                                            nlp.data.batchify.Stack(dtype='float32'),\n",
    "                                            nlp.data.batchify.Stack(dtype='float32'))\n",
    "test_batchify_fn = nlp.data.batchify.Tuple(nlp.data.batchify.Pad(),\n",
    "                                           nlp.data.batchify.Pad(),\n",
    "                                           nlp.data.batchify.Stack(dtype='float32'),\n",
    "                                           nlp.data.batchify.Stack(dtype='float32'),\n",
    "                                           nlp.data.batchify.Stack())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then construct bucketing samplers, which generate batches by grouping\n",
    "sequences with similar lengths. Here, the bucketing scheme is empirically determined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:07:01,404 - root - Train Batch Sampler:\n",
      "FixedBucketSampler:\n",
      "  sample_num=133166, batch_num=1043\n",
      "  key=[(9, 10), (16, 17), (26, 27), (37, 38), (51, 52)]\n",
      "  cnt=[11414, 34897, 37760, 23480, 25615]\n",
      "  batch_size=[128, 128, 128, 128, 128]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:07:01,407 - root - Valid Batch Sampler:\n",
      "FixedBucketSampler:\n",
      "  sample_num=1553, batch_num=52\n",
      "  key=[(22, 28), (40, 52), (58, 76), (76, 100), (94, 124)]\n",
      "  cnt=[1037, 432, 67, 10, 7]\n",
      "  batch_size=[32, 32, 32, 32, 32]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:07:01,410 - root - Test Batch Sampler:\n",
      "FixedBucketSampler:\n",
      "  sample_num=1268, batch_num=42\n",
      "  key=[(23, 29), (43, 53), (63, 77), (83, 101), (103, 125)]\n",
      "  cnt=[770, 381, 84, 26, 7]\n",
      "  batch_size=[32, 32, 32, 32, 32]\n"
     ]
    }
   ],
   "source": [
    "bucket_scheme = nlp.data.ExpWidthBucket(bucket_len_step=1.2)\n",
    "train_batch_sampler = nlp.data.FixedBucketSampler(lengths=data_train_lengths,\n",
    "                                                  batch_size=batch_size,\n",
    "                                                  num_buckets=num_buckets,\n",
    "                                                  shuffle=True,\n",
    "                                                  bucket_scheme=bucket_scheme)\n",
    "logging.info('Train Batch Sampler:\\n{}'.format(train_batch_sampler.stats()))\n",
    "val_batch_sampler = nlp.data.FixedBucketSampler(lengths=data_val_lengths,\n",
    "                                                batch_size=test_batch_size,\n",
    "                                                num_buckets=num_buckets,\n",
    "                                                shuffle=False)\n",
    "logging.info('Valid Batch Sampler:\\n{}'.format(val_batch_sampler.stats()))\n",
    "test_batch_sampler = nlp.data.FixedBucketSampler(lengths=data_test_lengths,\n",
    "                                                 batch_size=test_batch_size,\n",
    "                                                 num_buckets=num_buckets,\n",
    "                                                 shuffle=False)\n",
    "logging.info('Test Batch Sampler:\\n{}'.format(test_batch_sampler.stats()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the samplers, we can create DataLoader, which is iterable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader = gluon.data.DataLoader(data_train,\n",
    "                                          batch_sampler=train_batch_sampler,\n",
    "                                          batchify_fn=train_batchify_fn,\n",
    "                                          num_workers=4)\n",
    "val_data_loader = gluon.data.DataLoader(data_val,\n",
    "                                        batch_sampler=val_batch_sampler,\n",
    "                                        batchify_fn=test_batchify_fn,\n",
    "                                        num_workers=4)\n",
    "test_data_loader = gluon.data.DataLoader(data_test,\n",
    "                                         batch_sampler=test_batch_sampler,\n",
    "                                         batchify_fn=test_batchify_fn,\n",
    "                                         num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build GNMT Model\n",
    "\n",
    "After obtaining DataLoader, we can build the model. The GNMT encoder and decoder\n",
    "can be easily constructed by calling `get_gnmt_encoder_decoder` function. Then, we\n",
    "feed the encoder and decoder to `NMTModel` to construct the GNMT model.\n",
    "`model.hybridize` allows computation to be done using the symbolic backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:07:10,628 - root - NMTModel(\n",
      "  (encoder): GNMTEncoder(\n",
      "    (dropout_layer): Dropout(p = 0.2, axes=())\n",
      "    (rnn_cells): HybridSequential(\n",
      "      (0): BidirectionalCell(forward=LSTMCell(None -> 2048), backward=LSTMCell(None -> 2048))\n",
      "      (1): LSTMCell(None -> 2048)\n",
      "    )\n",
      "  )\n",
      "  (decoder): GNMTDecoder(\n",
      "    (attention_cell): DotProductAttentionCell(\n",
      "      (_dropout_layer): Dropout(p = 0.0, axes=())\n",
      "      (_proj_query): Dense(None -> 512, linear)\n",
      "    )\n",
      "    (dropout_layer): Dropout(p = 0.2, axes=())\n",
      "    (rnn_cells): HybridSequential(\n",
      "      (0): LSTMCell(None -> 2048)\n",
      "      (1): LSTMCell(None -> 2048)\n",
      "    )\n",
      "  )\n",
      "  (src_embed): HybridSequential(\n",
      "    (0): Embedding(17191 -> 512, float32)\n",
      "    (1): Dropout(p = 0.0, axes=())\n",
      "  )\n",
      "  (tgt_embed): HybridSequential(\n",
      "    (0): Embedding(7709 -> 512, float32)\n",
      "    (1): Dropout(p = 0.0, axes=())\n",
      "  )\n",
      "  (tgt_proj): Dense(None -> 7709, linear)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "encoder, decoder = nmt.gnmt.get_gnmt_encoder_decoder(hidden_size=num_hidden,\n",
    "                                                     dropout=dropout,\n",
    "                                                     num_layers=num_layers,\n",
    "                                                     num_bi_layers=num_bi_layers)\n",
    "model = nlp.model.translation.NMTModel(src_vocab=src_vocab, tgt_vocab=tgt_vocab, encoder=encoder,\n",
    "                                       decoder=decoder, embed_size=num_hidden, prefix='gnmt_')\n",
    "model.initialize(init=mx.init.Uniform(0.1), ctx=ctx)\n",
    "static_alloc = True\n",
    "model.hybridize(static_alloc=static_alloc)\n",
    "logging.info(model)\n",
    "\n",
    "# Due to the paddings, we need to mask out the losses corresponding to padding tokens.\n",
    "loss_function = nlp.loss.MaskedSoftmaxCELoss()\n",
    "loss_function.hybridize(static_alloc=static_alloc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also build the beam search translator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:07:10,726 - root - Use beam_size=10, alpha=1.0, K=5\n"
     ]
    }
   ],
   "source": [
    "translator = nmt.translation.BeamSearchTranslator(model=model, beam_size=beam_size,\n",
    "                                                  scorer=nlp.model.BeamSearchScorer(alpha=lp_alpha,\n",
    "                                                                                    K=lp_k),\n",
    "                                                  max_length=tgt_max_len + 100)\n",
    "logging.info('Use beam_size={}, alpha={}, K={}'.format(beam_size, lp_alpha, lp_k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define evaluation function as follows. The `evaluate` function use beam\n",
    "search translator to generate outputs for the validation and testing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_loader):\n",
    "    \"\"\"Evaluate given the data loader\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_loader : gluon.data.DataLoader\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    avg_loss : float\n",
    "        Average loss\n",
    "    real_translation_out : list of list of str\n",
    "        The translation output\n",
    "    \"\"\"\n",
    "    translation_out = []\n",
    "    all_inst_ids = []\n",
    "    avg_loss_denom = 0\n",
    "    avg_loss = 0.0\n",
    "    for _, (src_seq, tgt_seq, src_valid_length, tgt_valid_length, inst_ids) \\\n",
    "            in enumerate(data_loader):\n",
    "        src_seq = src_seq.as_in_context(ctx)\n",
    "        tgt_seq = tgt_seq.as_in_context(ctx)\n",
    "        src_valid_length = src_valid_length.as_in_context(ctx)\n",
    "        tgt_valid_length = tgt_valid_length.as_in_context(ctx)\n",
    "        # Calculating Loss\n",
    "        out, _ = model(src_seq, tgt_seq[:, :-1], src_valid_length, tgt_valid_length - 1)\n",
    "        loss = loss_function(out, tgt_seq[:, 1:], tgt_valid_length - 1).mean().asscalar()\n",
    "        all_inst_ids.extend(inst_ids.asnumpy().astype(np.int32).tolist())\n",
    "        avg_loss += loss * (tgt_seq.shape[1] - 1)\n",
    "        avg_loss_denom += (tgt_seq.shape[1] - 1)\n",
    "        # Translate\n",
    "        samples, _, sample_valid_length =\\\n",
    "            translator.translate(src_seq=src_seq, src_valid_length=src_valid_length)\n",
    "        max_score_sample = samples[:, 0, :].asnumpy()\n",
    "        sample_valid_length = sample_valid_length[:, 0].asnumpy()\n",
    "        for i in range(max_score_sample.shape[0]):\n",
    "            translation_out.append(\n",
    "                [tgt_vocab.idx_to_token[ele] for ele in\n",
    "                 max_score_sample[i][1:(sample_valid_length[i] - 1)]])\n",
    "    avg_loss = avg_loss / avg_loss_denom\n",
    "    real_translation_out = [None for _ in range(len(all_inst_ids))]\n",
    "    for ind, sentence in zip(all_inst_ids, translation_out):\n",
    "        real_translation_out[ind] = sentence\n",
    "    return avg_loss, real_translation_out\n",
    "\n",
    "\n",
    "def write_sentences(sentences, file_path):\n",
    "    with io.open(file_path, 'w', encoding='utf-8') as of:\n",
    "        for sent in sentences:\n",
    "            of.write(' '.join(sent) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Epochs\n",
    "\n",
    "Before entering the training stage, we need to create trainer for updating the\n",
    "parameters. In the following example, we create a trainer that uses ADAM\n",
    "optimzier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = gluon.Trainer(model.collect_params(), 'adam', {'learning_rate': lr})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then write the training loop. During the training, we evaluate on the validation and testing datasets every epoch, and record the\n",
    "parameters that give the hightest BLEU score on the validation dataset. Before\n",
    "performing forward and backward, we first use `as_in_context` function to copy\n",
    "the mini-batch to GPU. The statement `with mx.autograd.record()` tells Gluon\n",
    "backend to compute the gradients for the part inside the block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:07:15,877 - root - [Epoch 0 Batch 10/1043] loss=7.7375, ppl=2292.6586, gnorm=1.4907, throughput=10.79K wps, wc=54.27K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:07:18,550 - root - [Epoch 0 Batch 20/1043] loss=6.3590, ppl=577.6408, gnorm=1.5744, throughput=20.18K wps, wc=50.20K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:07:21,838 - root - [Epoch 0 Batch 30/1043] loss=6.3708, ppl=584.5346, gnorm=0.8043, throughput=20.65K wps, wc=67.78K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:07:24,862 - root - [Epoch 0 Batch 40/1043] loss=6.1792, ppl=482.5820, gnorm=0.6213, throughput=20.91K wps, wc=63.19K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:07:27,934 - root - [Epoch 0 Batch 50/1043] loss=6.1871, ppl=486.4455, gnorm=0.4034, throughput=20.18K wps, wc=61.93K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:07:30,781 - root - [Epoch 0 Batch 60/1043] loss=6.1060, ppl=448.5210, gnorm=0.6787, throughput=20.81K wps, wc=59.19K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:07:34,186 - root - [Epoch 0 Batch 70/1043] loss=6.1569, ppl=471.9777, gnorm=0.4684, throughput=21.45K wps, wc=72.99K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:07:37,342 - root - [Epoch 0 Batch 80/1043] loss=6.0705, ppl=432.9092, gnorm=0.4163, throughput=20.49K wps, wc=64.58K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:07:39,998 - root - [Epoch 0 Batch 90/1043] loss=5.9389, ppl=379.4997, gnorm=0.3626, throughput=20.00K wps, wc=53.02K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:07:42,745 - root - [Epoch 0 Batch 100/1043] loss=5.8775, ppl=356.9234, gnorm=0.4090, throughput=21.65K wps, wc=59.42K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:07:45,672 - root - [Epoch 0 Batch 110/1043] loss=5.8723, ppl=355.0713, gnorm=0.3617, throughput=22.40K wps, wc=65.50K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:07:48,628 - root - [Epoch 0 Batch 120/1043] loss=5.8694, ppl=354.0462, gnorm=0.3479, throughput=19.78K wps, wc=58.43K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:07:52,035 - root - [Epoch 0 Batch 130/1043] loss=5.9339, ppl=377.6277, gnorm=0.3579, throughput=17.45K wps, wc=59.39K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:07:54,998 - root - [Epoch 0 Batch 140/1043] loss=5.8808, ppl=358.1109, gnorm=0.2855, throughput=20.67K wps, wc=61.18K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:07:57,846 - root - [Epoch 0 Batch 150/1043] loss=5.8124, ppl=334.4147, gnorm=0.2918, throughput=19.81K wps, wc=56.34K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:08:00,927 - root - [Epoch 0 Batch 160/1043] loss=5.7398, ppl=311.0072, gnorm=0.3737, throughput=18.82K wps, wc=57.93K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:08:03,896 - root - [Epoch 0 Batch 170/1043] loss=5.7608, ppl=317.6178, gnorm=0.3004, throughput=21.72K wps, wc=64.42K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:08:06,147 - root - [Epoch 0 Batch 180/1043] loss=5.4566, ppl=234.2920, gnorm=0.3406, throughput=19.71K wps, wc=44.31K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:08:09,172 - root - [Epoch 0 Batch 190/1043] loss=5.6080, ppl=272.6107, gnorm=0.3578, throughput=20.66K wps, wc=62.45K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:08:11,945 - root - [Epoch 0 Batch 200/1043] loss=5.5012, ppl=244.9762, gnorm=0.3381, throughput=19.59K wps, wc=54.24K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:08:14,550 - root - [Epoch 0 Batch 210/1043] loss=5.3199, ppl=204.3678, gnorm=0.4236, throughput=20.24K wps, wc=52.68K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:08:16,975 - root - [Epoch 0 Batch 220/1043] loss=5.3090, ppl=202.1534, gnorm=0.3807, throughput=20.85K wps, wc=50.47K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:08:20,035 - root - [Epoch 0 Batch 230/1043] loss=5.3090, ppl=202.1443, gnorm=0.4230, throughput=20.16K wps, wc=61.65K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:08:22,727 - root - [Epoch 0 Batch 240/1043] loss=5.3017, ppl=200.6791, gnorm=0.3476, throughput=21.69K wps, wc=58.32K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:08:26,094 - root - [Epoch 0 Batch 250/1043] loss=5.4215, ppl=226.2137, gnorm=0.2882, throughput=21.06K wps, wc=70.84K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:08:28,930 - root - [Epoch 0 Batch 260/1043] loss=5.2739, ppl=195.1772, gnorm=0.3596, throughput=21.26K wps, wc=60.22K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:08:32,152 - root - [Epoch 0 Batch 270/1043] loss=5.3786, ppl=216.7152, gnorm=0.2841, throughput=22.66K wps, wc=72.96K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:08:35,199 - root - [Epoch 0 Batch 280/1043] loss=5.2528, ppl=191.0951, gnorm=0.2658, throughput=19.99K wps, wc=60.80K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:08:37,725 - root - [Epoch 0 Batch 290/1043] loss=4.9769, ppl=145.0176, gnorm=0.3470, throughput=18.15K wps, wc=45.79K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:08:40,539 - root - [Epoch 0 Batch 300/1043] loss=5.0869, ppl=161.8902, gnorm=0.3500, throughput=21.01K wps, wc=59.05K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:08:43,563 - root - [Epoch 0 Batch 310/1043] loss=5.1068, ppl=165.1394, gnorm=0.3009, throughput=20.39K wps, wc=61.58K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:08:46,201 - root - [Epoch 0 Batch 320/1043] loss=4.9811, ppl=145.6305, gnorm=0.3154, throughput=20.16K wps, wc=53.10K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:08:49,139 - root - [Epoch 0 Batch 330/1043] loss=5.0422, ppl=154.8146, gnorm=0.3015, throughput=20.94K wps, wc=61.37K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:08:51,977 - root - [Epoch 0 Batch 340/1043] loss=5.0655, ppl=158.4552, gnorm=0.2632, throughput=20.06K wps, wc=56.88K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:08:54,645 - root - [Epoch 0 Batch 350/1043] loss=4.9246, ppl=137.6404, gnorm=0.3163, throughput=20.60K wps, wc=54.86K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:08:57,510 - root - [Epoch 0 Batch 360/1043] loss=5.0421, ppl=154.7988, gnorm=0.2850, throughput=22.55K wps, wc=64.55K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:09:00,589 - root - [Epoch 0 Batch 370/1043] loss=4.9064, ppl=135.1580, gnorm=0.2956, throughput=21.78K wps, wc=66.97K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:09:03,343 - root - [Epoch 0 Batch 380/1043] loss=4.8172, ppl=123.6220, gnorm=0.3044, throughput=19.21K wps, wc=52.79K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:09:05,976 - root - [Epoch 0 Batch 390/1043] loss=4.7893, ppl=120.2213, gnorm=0.3350, throughput=19.36K wps, wc=50.94K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:09:08,424 - root - [Epoch 0 Batch 400/1043] loss=4.6555, ppl=105.1638, gnorm=0.3907, throughput=19.72K wps, wc=48.22K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:09:10,901 - root - [Epoch 0 Batch 410/1043] loss=4.8339, ppl=125.6981, gnorm=0.3242, throughput=19.51K wps, wc=48.27K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:09:13,765 - root - [Epoch 0 Batch 420/1043] loss=4.8519, ppl=127.9888, gnorm=0.3055, throughput=19.62K wps, wc=56.14K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:09:17,052 - root - [Epoch 0 Batch 430/1043] loss=4.8984, ppl=134.0705, gnorm=0.2895, throughput=21.11K wps, wc=69.33K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:09:20,013 - root - [Epoch 0 Batch 440/1043] loss=4.7876, ppl=120.0098, gnorm=0.2734, throughput=22.68K wps, wc=67.08K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:09:22,586 - root - [Epoch 0 Batch 450/1043] loss=4.6916, ppl=109.0252, gnorm=0.3080, throughput=20.88K wps, wc=53.68K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:09:25,063 - root - [Epoch 0 Batch 460/1043] loss=4.4769, ppl=87.9586, gnorm=0.3489, throughput=20.38K wps, wc=50.38K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:09:28,053 - root - [Epoch 0 Batch 470/1043] loss=4.7561, ppl=116.2918, gnorm=0.2942, throughput=20.32K wps, wc=60.70K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:09:30,475 - root - [Epoch 0 Batch 480/1043] loss=4.3967, ppl=81.1856, gnorm=0.3453, throughput=22.34K wps, wc=54.04K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:09:32,701 - root - [Epoch 0 Batch 490/1043] loss=4.5094, ppl=90.8703, gnorm=0.4202, throughput=20.83K wps, wc=46.32K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:09:35,108 - root - [Epoch 0 Batch 500/1043] loss=4.5656, ppl=96.1205, gnorm=0.3105, throughput=20.18K wps, wc=48.55K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:09:37,246 - root - [Epoch 0 Batch 510/1043] loss=4.3288, ppl=75.8559, gnorm=0.3366, throughput=19.49K wps, wc=41.62K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:09:39,319 - root - [Epoch 0 Batch 520/1043] loss=4.2394, ppl=69.3683, gnorm=0.3773, throughput=19.31K wps, wc=39.98K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:09:42,146 - root - [Epoch 0 Batch 530/1043] loss=4.6554, ppl=105.1503, gnorm=0.3106, throughput=20.74K wps, wc=58.58K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:09:44,566 - root - [Epoch 0 Batch 540/1043] loss=4.5210, ppl=91.9245, gnorm=0.3208, throughput=20.97K wps, wc=50.72K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:09:47,482 - root - [Epoch 0 Batch 550/1043] loss=4.5447, ppl=94.1324, gnorm=0.3214, throughput=21.61K wps, wc=62.95K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:09:49,749 - root - [Epoch 0 Batch 560/1043] loss=4.3290, ppl=75.8656, gnorm=0.3394, throughput=20.38K wps, wc=46.13K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:09:52,521 - root - [Epoch 0 Batch 570/1043] loss=4.3493, ppl=77.4252, gnorm=0.3009, throughput=22.07K wps, wc=61.12K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:09:55,029 - root - [Epoch 0 Batch 580/1043] loss=4.3643, ppl=78.5975, gnorm=0.3134, throughput=22.13K wps, wc=55.43K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:09:58,126 - root - [Epoch 0 Batch 590/1043] loss=4.5888, ppl=98.3770, gnorm=0.2748, throughput=23.89K wps, wc=73.93K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:10:00,659 - root - [Epoch 0 Batch 600/1043] loss=4.4959, ppl=89.6519, gnorm=0.2772, throughput=22.04K wps, wc=55.80K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:10:03,092 - root - [Epoch 0 Batch 610/1043] loss=4.3174, ppl=74.9904, gnorm=0.3306, throughput=21.51K wps, wc=52.28K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:10:06,252 - root - [Epoch 0 Batch 620/1043] loss=4.5597, ppl=95.5579, gnorm=0.2581, throughput=22.92K wps, wc=72.39K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:10:08,034 - root - [Epoch 0 Batch 630/1043] loss=4.0495, ppl=57.3701, gnorm=0.3413, throughput=19.34K wps, wc=34.44K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:10:10,518 - root - [Epoch 0 Batch 640/1043] loss=4.3356, ppl=76.3704, gnorm=0.3157, throughput=23.10K wps, wc=57.32K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:10:13,542 - root - [Epoch 0 Batch 650/1043] loss=4.4767, ppl=87.9457, gnorm=0.2783, throughput=21.99K wps, wc=66.42K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:10:15,669 - root - [Epoch 0 Batch 660/1043] loss=4.1965, ppl=66.4513, gnorm=0.3497, throughput=20.90K wps, wc=44.42K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:10:18,980 - root - [Epoch 0 Batch 670/1043] loss=4.6035, ppl=99.8329, gnorm=0.2592, throughput=23.48K wps, wc=77.68K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:10:21,669 - root - [Epoch 0 Batch 680/1043] loss=4.4282, ppl=83.7837, gnorm=0.2850, throughput=21.97K wps, wc=59.02K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:10:23,997 - root - [Epoch 0 Batch 690/1043] loss=4.2100, ppl=67.3599, gnorm=0.3212, throughput=21.74K wps, wc=50.54K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:10:26,418 - root - [Epoch 0 Batch 700/1043] loss=4.2986, ppl=73.5946, gnorm=0.3053, throughput=21.69K wps, wc=52.45K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:10:28,419 - root - [Epoch 0 Batch 710/1043] loss=4.1867, ppl=65.8024, gnorm=0.3388, throughput=20.67K wps, wc=41.32K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:10:30,725 - root - [Epoch 0 Batch 720/1043] loss=4.2639, ppl=71.0879, gnorm=0.3010, throughput=21.74K wps, wc=50.09K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:10:33,270 - root - [Epoch 0 Batch 730/1043] loss=4.1942, ppl=66.3012, gnorm=0.3244, throughput=20.47K wps, wc=52.02K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:10:36,032 - root - [Epoch 0 Batch 740/1043] loss=4.3311, ppl=76.0264, gnorm=0.2705, throughput=21.69K wps, wc=59.86K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:10:38,592 - root - [Epoch 0 Batch 750/1043] loss=4.1912, ppl=66.0998, gnorm=0.2909, throughput=20.87K wps, wc=53.35K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:10:41,619 - root - [Epoch 0 Batch 760/1043] loss=4.2881, ppl=72.8249, gnorm=0.2956, throughput=23.18K wps, wc=70.09K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:10:43,929 - root - [Epoch 0 Batch 770/1043] loss=4.1163, ppl=61.3322, gnorm=0.3184, throughput=18.73K wps, wc=43.21K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:10:47,341 - root - [Epoch 0 Batch 780/1043] loss=4.3126, ppl=74.6316, gnorm=0.2868, throughput=22.26K wps, wc=75.93K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:10:49,706 - root - [Epoch 0 Batch 790/1043] loss=4.1421, ppl=62.9344, gnorm=0.3124, throughput=19.83K wps, wc=46.81K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:10:52,456 - root - [Epoch 0 Batch 800/1043] loss=4.2065, ppl=67.1220, gnorm=0.3203, throughput=21.38K wps, wc=58.72K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:10:55,154 - root - [Epoch 0 Batch 810/1043] loss=4.0828, ppl=59.3103, gnorm=0.3072, throughput=21.30K wps, wc=57.38K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:10:57,881 - root - [Epoch 0 Batch 820/1043] loss=3.9687, ppl=52.9133, gnorm=0.3280, throughput=21.48K wps, wc=58.52K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:11:00,641 - root - [Epoch 0 Batch 830/1043] loss=4.1222, ppl=61.6933, gnorm=0.3378, throughput=20.76K wps, wc=57.24K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:11:03,151 - root - [Epoch 0 Batch 840/1043] loss=4.0870, ppl=59.5630, gnorm=0.3098, throughput=20.97K wps, wc=52.57K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:11:06,283 - root - [Epoch 0 Batch 850/1043] loss=4.1362, ppl=62.5645, gnorm=0.2948, throughput=20.49K wps, wc=64.14K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:11:08,865 - root - [Epoch 0 Batch 860/1043] loss=4.1289, ppl=62.1117, gnorm=0.2949, throughput=21.20K wps, wc=54.64K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:11:11,793 - root - [Epoch 0 Batch 870/1043] loss=4.1557, ppl=63.7995, gnorm=0.3035, throughput=22.50K wps, wc=65.81K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:11:14,319 - root - [Epoch 0 Batch 880/1043] loss=4.0874, ppl=59.5845, gnorm=0.2924, throughput=19.94K wps, wc=50.27K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:11:16,907 - root - [Epoch 0 Batch 890/1043] loss=4.1518, ppl=63.5505, gnorm=0.2808, throughput=21.70K wps, wc=56.11K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:11:19,768 - root - [Epoch 0 Batch 900/1043] loss=4.1750, ppl=65.0426, gnorm=0.2873, throughput=21.32K wps, wc=60.91K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:11:22,188 - root - [Epoch 0 Batch 910/1043] loss=3.9803, ppl=53.5334, gnorm=0.3152, throughput=21.36K wps, wc=51.65K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:11:24,957 - root - [Epoch 0 Batch 920/1043] loss=4.1676, ppl=64.5620, gnorm=0.2911, throughput=21.89K wps, wc=60.52K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:11:27,247 - root - [Epoch 0 Batch 930/1043] loss=3.9696, ppl=52.9634, gnorm=0.3005, throughput=19.03K wps, wc=43.51K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:11:29,616 - root - [Epoch 0 Batch 940/1043] loss=3.9258, ppl=50.6934, gnorm=0.3263, throughput=21.00K wps, wc=49.71K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:11:32,727 - root - [Epoch 0 Batch 950/1043] loss=4.2013, ppl=66.7743, gnorm=0.2856, throughput=22.81K wps, wc=70.92K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:11:36,016 - root - [Epoch 0 Batch 960/1043] loss=4.1524, ppl=63.5859, gnorm=0.2733, throughput=22.53K wps, wc=74.06K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:11:38,165 - root - [Epoch 0 Batch 970/1043] loss=3.8736, ppl=48.1166, gnorm=0.3670, throughput=20.51K wps, wc=44.03K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:11:41,501 - root - [Epoch 0 Batch 980/1043] loss=4.1779, ppl=65.2289, gnorm=0.2841, throughput=21.81K wps, wc=72.73K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:11:44,488 - root - [Epoch 0 Batch 990/1043] loss=4.0953, ppl=60.0574, gnorm=0.3038, throughput=21.91K wps, wc=65.39K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:11:56,178 - root - [Epoch 0 Batch 1000/1043] loss=3.9916, ppl=54.1427, gnorm=0.3146, throughput=4.76K wps, wc=55.65K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:11:59,350 - root - [Epoch 0 Batch 1010/1043] loss=4.0738, ppl=58.7818, gnorm=0.2859, throughput=20.71K wps, wc=65.61K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:12:01,588 - root - [Epoch 0 Batch 1020/1043] loss=3.8215, ppl=45.6737, gnorm=0.3306, throughput=20.45K wps, wc=45.72K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:12:04,313 - root - [Epoch 0 Batch 1030/1043] loss=3.9710, ppl=53.0402, gnorm=0.2997, throughput=20.56K wps, wc=55.97K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:12:06,833 - root - [Epoch 0 Batch 1040/1043] loss=3.9365, ppl=51.2394, gnorm=0.3537, throughput=21.16K wps, wc=53.27K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:12:43,896 - root - [Epoch 0] valid Loss=2.8391, valid ppl=17.1007, valid bleu=3.03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:13:05,255 - root - [Epoch 0] test Loss=2.9800, test ppl=19.6873, test bleu=2.94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:13:05,269 - root - Save best parameters to gnmt_en_vi_u512/valid_best.params\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:13:05,575 - root - Learning rate change to 0.0005\n"
     ]
    }
   ],
   "source": [
    "best_valid_bleu = 0.0\n",
    "for epoch_id in range(epochs):\n",
    "    log_avg_loss = 0\n",
    "    log_avg_gnorm = 0\n",
    "    log_wc = 0\n",
    "    log_start_time = time.time()\n",
    "    for batch_id, (src_seq, tgt_seq, src_valid_length, tgt_valid_length)\\\n",
    "            in enumerate(train_data_loader):\n",
    "        # logging.info(src_seq.context) Context suddenly becomes GPU.\n",
    "        src_seq = src_seq.as_in_context(ctx)\n",
    "        tgt_seq = tgt_seq.as_in_context(ctx)\n",
    "        src_valid_length = src_valid_length.as_in_context(ctx)\n",
    "        tgt_valid_length = tgt_valid_length.as_in_context(ctx)\n",
    "        with mx.autograd.record():\n",
    "            out, _ = model(src_seq, tgt_seq[:, :-1], src_valid_length, tgt_valid_length - 1)\n",
    "            loss = loss_function(out, tgt_seq[:, 1:], tgt_valid_length - 1).mean()\n",
    "            loss = loss * (tgt_seq.shape[1] - 1) / (tgt_valid_length - 1).mean()\n",
    "            loss.backward()\n",
    "        grads = [p.grad(ctx) for p in model.collect_params().values()]\n",
    "        gnorm = gluon.utils.clip_global_norm(grads, clip)\n",
    "        trainer.step(1)\n",
    "        src_wc = src_valid_length.sum().asscalar()\n",
    "        tgt_wc = (tgt_valid_length - 1).sum().asscalar()\n",
    "        step_loss = loss.asscalar()\n",
    "        log_avg_loss += step_loss\n",
    "        log_avg_gnorm += gnorm\n",
    "        log_wc += src_wc + tgt_wc\n",
    "        if (batch_id + 1) % log_interval == 0:\n",
    "            wps = log_wc / (time.time() - log_start_time)\n",
    "            logging.info('[Epoch {} Batch {}/{}] loss={:.4f}, ppl={:.4f}, gnorm={:.4f}, '\n",
    "                         'throughput={:.2f}K wps, wc={:.2f}K'\n",
    "                         .format(epoch_id, batch_id + 1, len(train_data_loader),\n",
    "                                 log_avg_loss / log_interval,\n",
    "                                 np.exp(log_avg_loss / log_interval),\n",
    "                                 log_avg_gnorm / log_interval,\n",
    "                                 wps / 1000, log_wc / 1000))\n",
    "            log_start_time = time.time()\n",
    "            log_avg_loss = 0\n",
    "            log_avg_gnorm = 0\n",
    "            log_wc = 0\n",
    "    valid_loss, valid_translation_out = evaluate(val_data_loader)\n",
    "    valid_bleu_score, _, _, _, _ = nmt.bleu.compute_bleu([val_tgt_sentences], valid_translation_out)\n",
    "    logging.info('[Epoch {}] valid Loss={:.4f}, valid ppl={:.4f}, valid bleu={:.2f}'\n",
    "                 .format(epoch_id, valid_loss, np.exp(valid_loss), valid_bleu_score * 100))\n",
    "    test_loss, test_translation_out = evaluate(test_data_loader)\n",
    "    test_bleu_score, _, _, _, _ = nmt.bleu.compute_bleu([test_tgt_sentences], test_translation_out)\n",
    "    logging.info('[Epoch {}] test Loss={:.4f}, test ppl={:.4f}, test bleu={:.2f}'\n",
    "                 .format(epoch_id, test_loss, np.exp(test_loss), test_bleu_score * 100))\n",
    "    write_sentences(valid_translation_out,\n",
    "                    os.path.join(save_dir, 'epoch{:d}_valid_out.txt').format(epoch_id))\n",
    "    write_sentences(test_translation_out,\n",
    "                    os.path.join(save_dir, 'epoch{:d}_test_out.txt').format(epoch_id))\n",
    "    if valid_bleu_score > best_valid_bleu:\n",
    "        best_valid_bleu = valid_bleu_score\n",
    "        save_path = os.path.join(save_dir, 'valid_best.params')\n",
    "        logging.info('Save best parameters to {}'.format(save_path))\n",
    "        model.save_parameters(save_path)\n",
    "    if epoch_id + 1 >= (epochs * 2) // 3:\n",
    "        new_lr = trainer.learning_rate * lr_update_factor\n",
    "        logging.info('Learning rate change to {}'.format(new_lr))\n",
    "        trainer.set_learning_rate(new_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "In this notebook, we have shown how to train a GNMT model on IWSLT 2015 English-Vietnamese using Gluon NLP toolkit.\n",
    "The complete training script can be found [here](https://github.com/dmlc/gluon-nlp/blob/master/scripts/machine_translation/train_gnmt.py).\n",
    "The command to reproduce the result can be seen in the [machine translation page](http://gluon-nlp.mxnet.io/model_zoo/machine_translation/index.html)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}